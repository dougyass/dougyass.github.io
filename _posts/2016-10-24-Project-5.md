<h1>Predicting Titanic Survival</h1>

<img src="https://cdn29.elitedaily.com/content/uploads/2016/04/18124655/elite-daily-titanic.jpg">

<h2>Problem Statement</h2>
<p>Given data from passengers on the titanic, predict which variables predict survival.</p>

<h2>Risks and Assumptions</h2>
<p>We worked with a data set of 712 passengers, but there were 2,200 people on the titanic. We are also assuming we are working with an accurate data set.  </p>


<h2>Data Analysis</h2>
<p>
We were orginally given the cabin, ticket, class, name, sex, age, total siblings, total children, fare, and embarking point for passengers on the titanic. I ultimately decided to break down the data so it predicted survival rate based on their title (Mr., Mrs., Miss., etc.), sex, embarking point, class, siblings, children, fare, and age. After creating dummies for each of those variables I dropped a base case for each and began creating a model.
</p>

<p>
	The model showed you had a higher chance of survial if you were female, higher class, had less siblings, had less children, and were over 80 or a child.
</p> 

<p>
The logistic regression model had a cross-val score of 0.80, a precision score of 0.78, a recall of 0.78, and an f1-score of 0.77. The confusion matrix correctly predicted 80% of survivors and 74% of non-survivors. 
</p>
<img src="http://i65.tinypic.com/mijaea.png">

<p>
</p>
<p>
The ROC-AUC had an area of 0.84 as seen below. From eyeballing the graph it looks like the most accurate point was around when the false positive rate was 0.30 and the true positive rate was 0.82.

<img src="http://i68.tinypic.com/snlxsm.png"> 
	
	
</p>

<p>
I then ran a GridSearch to see what the best parameters were for the model and found that a C of 5.0 and an l2 penalty were the best parameters. Running the model with the best parameters, 83% of survivors were predicted correctly and 74% of survivors were predicted correctly, but it still had an f1-score of 0.79.

</p>



<p>
	Then using Gridsearch and kNN we found that our optimal model would use 10 neighbors and saw we had a similar f1-score with the kNN approach.


</p>


<h2> Conclusion</h2>

<p>
	Whether it was a lasso, a ridge, or a kNN model the model always had an f1-score in the high seventies. Some of the coefficients in the model were very high, but they also had very high standard errors. For example, the model had a positive coefficient of 18.59 if the passenger had the title "Sir", but the standard error was 8552. The model predicted what was expected. Women, people in first class, and children had higher rates of survival.
</p>









